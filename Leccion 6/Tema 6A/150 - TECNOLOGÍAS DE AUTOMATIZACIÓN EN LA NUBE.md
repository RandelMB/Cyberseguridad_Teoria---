


Infraestructura como código
Infraestructura como código (IaC) es una práctica de ingeniería de software que administra la infraestructura informática mediante archivos de definición legibles por máquina. Estos archivos contienen código escrito en un formato específico que las máquinas pueden leer y ejecutar. Administran y aprovisionan la infraestructura de computación. Los archivos de definición legibles por máquina se escriben en formatos como YAML, JSON y HCL (lenguaje de configuración de HashiCorp). Contienen información sobre el estado de la infraestructura deseada, incluidos los ajustes de configuración, los requisitos de red, las políticas de seguridad y otros ajustes. Mediante el uso de archivos de definición legibles por máquina, la infraestructura se puede implementar y gestionar de forma automática y coherente, lo que reduce el riesgo de errores causados por la intervención manual. 

Estos archivos suelen estar controlados por versiones y pueden tratarse como cualquier otro código de un proyecto de software. La IaC permite a los desarrolladores y equipos de operaciones automatizar el proceso de implementación y gestión de la infraestructura, lo que reduce la probabilidad de errores e inconsistencias que pueden surgir de la configuración manual. Al utilizar IaC, los equipos también pueden replicar fácilmente la infraestructura en diferentes entornos, como el desarrollo, la puesta en escena y la producción, además de garantizar que la configuración de su infraestructura sea coherente y reproducible.

	HCL (HashiCorp Configuration Language) es un lenguaje de configuración desarrollado por HashiCorp y que se utiliza en entornos de Infraestructura como Código (IaC) para administrar y aprovisionar infraestructura informática. HCL es similar a JSON y YAML en términos de sintaxis, pero tiene algunas características adicionales que lo hacen más adecuado para la gestión de infraestructuras. Admite variables en los archivos de configuración y tiene una sintaxis concisa que facilita la lectura y la escritura. HCL se emplea en muchas herramientas populares de HashiCorp, incluidas Terraform y Consul.

Capacidad de respuesta
El equilibrio de carga, la computación periférica y el escalado automático son mecanismos fundamentales para garantizar la capacidad de respuesta, mejorar el rendimiento y manejar con eficacia las cargas de trabajo fluctuantes. 

- Equilibrio de carga: distribuye el tráfico de red a través de múltiples servidores o servicios para mejorar el rendimiento y proporcionar una alta disponibilidad. En la nube, los equilibradores de carga son intermediarios (proxies) entre los usuarios y los recursos de back-end, como máquinas virtuales o contenedores. Distribuyen las solicitudes entrantes a diferentes recursos mediante algoritmos sofisticados; también gestionan la capacidad del servidor, el tiempo de respuesta y la carga de trabajo. 
- Computación periférica: optimiza la ubicación geográfica de los recursos y servicios para permitir un procesamiento más rápido y una latencia reducida. En lugar de enrutar todos los datos a un centro de datos centralizado en la nube, la computación periférica utiliza recursos informáticos distribuidos para minimizar la distancia que los datos necesitan recorrer, lo que reduce la latencia de la red y mejora la capacidad de respuesta. La computación periférica es particularmente beneficiosa para aplicaciones que requieren procesamiento en tiempo real o de baja latencia, como dispositivos IoT, redes de entrega de contenido (CDN) y aplicaciones sensibles a la latencia.
- Escalado automático: es un proceso automatizado que ajusta los recursos informáticos asignados a una aplicación en función de la demanda. El escalado automático permite que la infraestructura de la nube adapte de manera dinámica los recursos hacia arriba o hacia abajo para que coincidan con los requisitos de carga de trabajo en tiempo real. Por ejemplo, durante los períodos de alta demanda, los recursos adicionales se aprovisionan automáticamente para manejar el aumento de la carga, lo que garantiza un rendimiento y una capacidad de respuesta óptimos. Por el contrario, cuando la demanda disminuye, los recursos innecesarios se vuelven a liberar en un grupo compartido para reducir los costos operativos o para ponerlos a disposición de otras cargas de trabajo. 
Estos mecanismos optimizan la utilización de recursos, reducen la latencia y permiten que la infraestructura se adapte a demanda a los patrones de carga de trabajo cambiantes, lo que genera un entorno de nube altamente receptivo y eficiente.